from gan.losses import GeneratorLoss, DiscriminatorLoss
import torch
from pathlib import Path


class GanTrainer(object):
    def __init__(self, generator, discriminator, config, noise_sampler, lr=0.0002, visualizer=None):
        """
        GanTrainer class can be used for training conditional or unconditional gan
        :param generator: generator network, takes z noise as input if unconditional and z, y if conditional
        :param discriminator: discriminator network, takes img as input if unconditional and img, y if conditional
        :param config:
        :param noise_sampler: class with sample_batch(batch_size) that returns (z,) tuple for unconditional gan
               and (z, y) tuple for conditional
        :param lr: learning rate
        :param visualizer: visualizer class that supports update_losses(g_loss, d_loss) and show_generator_results(generator)
        """
        self.generator = generator
        self.discriminator = discriminator
        self.config = config
        self.g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()),
                                            lr=lr, betas=(0.5, 0.999))
        self.d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()),
                                            lr=lr, betas=(0.5, 0.999))
        self.visualizer = visualizer
        self.current_epoch = 0
        self.noise_sampler = noise_sampler
        self.generator_template = 'generator_%d.pth'
        self.discriminator_template = 'discriminator_%d.pth'
        self.checkpoint_template = 'checkpoint_%d.pth'

    def train(self, loader, n_epochs):
        last_epoch = self.current_epoch
        for self.current_epoch in range(last_epoch + 1, n_epochs):
            self.train_epoch(loader)
            self.save_checkpoint()

    def load_last_checkpoint(self):
        model_path = self.config.MODEL_PATH
        last_epoch = self.get_last_checkpoint(model_path)
        if last_epoch >= 0:
            self.load_checkpoint(last_epoch)

    def train_epoch(self, loader):
        self.generator.train()
        self.discriminator.train()
        G_train_loss = 0.0
        D_train_loss = 0.0
        n_d_steps = 0
        n_g_steps = 0
        k_it = 0
        generator_loss = GeneratorLoss()
        discriminator_loss = DiscriminatorLoss(label_smoothing=self.config.label_smoothing)

        for sample in loader:
            # sample is (img,) tuple for regular gan
            # and (img, y) tuple for conditional gan
            self.d_optimizer.zero_grad()
            self.g_optimizer.zero_grad()

            D_real, D_logit_real = self.discriminator(*sample)

            # noise for unconditional gan is (z,) tuple with random noise vector from uniform distribution [-1, 1]
            # for conditional gan noise is (z, y) tuple where y is conditional vector defined by config.conditions
            noise = self.noise_sampler.sample_batch(loader.batch_size)
            # empty tuple for not conditional gan
            condition = noise[1:]

            # G_sample is batch of images generated by generator using noise as input
            G_sample = self.generator(*noise)
            D_fake, D_logit_fake = self.discriminator(G_sample, *condition)

            D_loss = discriminator_loss(D_logit_real, D_logit_fake)
            D_train_loss += D_loss.data

            D_loss.backward()
            self.d_optimizer.step()
            n_d_steps += 1

            k_it += 1

            if k_it == self.config.k:
                self.d_optimizer.zero_grad()
                self.g_optimizer.zero_grad()
                noise = self.noise_sampler.sample_batch(loader.batch_size)
                # empty tuple for not conditional gan
                condition = noise[1:]
                G_sample = self.generator(*noise)
                D_fake, D_logit_fake = self.discriminator(G_sample, *condition)
                G_loss = generator_loss(D_logit_fake)
                G_train_loss += G_loss.data

                G_loss.backward()
                self.g_optimizer.step()
                k_it = 0
                n_g_steps += 1

        if self.visualizer is not None:
            self.visualizer.update_losses(g_loss=G_train_loss / n_g_steps, d_loss=D_train_loss / n_d_steps,
                                          type='train')
            self.visualizer.show_generator_results(generator=self.generator)

    def save_checkpoint(self):
        """
        Save gan checkpoint for continuous training
        """

        save_path = Path(self.config.MODEL_PATH)
        save_path.mkdir(exist_ok=True)
        generator_path = self.generator_template % (self.current_epoch,)
        discriminator_path = self.discriminator_template % (self.current_epoch,)
        torch.save(self.generator.state_dict(), str(save_path / generator_path))
        torch.save(self.discriminator.state_dict(), str(save_path / discriminator_path))
        visdom_env = ""
        if self.visualizer is not None:
            visdom_env = self.visualizer.env_name
            self.visualizer.save()
        state = {
            'epoch': self.current_epoch,
            'generator': str(generator_path),
            'discriminator': str(discriminator_path),
            'g_optimizer': self.g_optimizer.state_dict(),
            'd_optimizer': self.d_optimizer.state_dict(),
            'visdom_env': visdom_env
        }
        checkpoint_path = self.checkpoint_template % (self.current_epoch,)
        torch.save(state, str(save_path / checkpoint_path))

    def load_checkpoint(self, epoch):
        """
        Load gan checkpoint for continuous training
        :param epoch: epoch to load
        """
        load_path = Path(self.config.MODEL_PATH)
        checkpoint_path = self.checkpoint_template % (epoch,)
        state = torch.load(str(load_path / checkpoint_path))
        epoch = state['epoch']
        generator_path = state['generator']
        discriminator_path = state['discriminator']
        self.generator.load_state_dict(torch.load(str(load_path / generator_path)))
        self.discriminator.load_state_dict(torch.load(str(load_path / discriminator_path)))
        self.g_optimizer.load_state_dict(state['g_optimizer'])
        self.d_optimizer.load_state_dict(state['d_optimizer'])
        visdom_env = state.get('visdom_env')
        self.current_epoch = epoch
        if self.visualizer is not None and visdom_env:
            self.visualizer.set_env(visdom_env)

    @staticmethod
    def get_last_checkpoint(path):
        path = Path(path)
        list_files = path.glob('checkpoint_*')
        epochs = [int(str(s).split('_')[-1].split('.')[0]) for s in list_files]
        if not epochs:
            return -1
        return max(epochs)

